{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "experimento2_dataset4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laughing-kazakhstan"
      },
      "source": [
        "### Importação dos pacotes"
      ],
      "id": "laughing-kazakhstan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "applicable-billion"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "applicable-billion",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worst-fisher"
      },
      "source": [
        "### Pré-processamento nos dados"
      ],
      "id": "worst-fisher"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regulation-costume"
      },
      "source": [
        "# lendo os dados de um arquivo csv\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
        "# criando uma coluna com os caminhos relativos as imagens\n",
        "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata_seg/' + dataframe.image_id + '.jpg'"
      ],
      "id": "regulation-costume",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "necessary-medication",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6ff736-2763-4313-f60f-f9337bf9006d"
      },
      "source": [
        "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
      ],
      "id": "necessary-medication",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de imagens disponíveis: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "widespread-enclosure",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73760a2b-26bc-4179-e31e-cfe4a1818b93"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "id": "widespread-enclosure",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Aortic enlargement     7162\n",
              "Cardiomegaly           5427\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Other lesion           2203\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "informal-ottawa"
      },
      "source": [
        "# removendo os casos não relativos a distúrbios pulmonares\n",
        "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
        "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
        "dataframe = dataframe[dataframe.class_name != 'Other lesion']\n",
        "dataframe = dataframe[dataframe.class_name != 'Consolidation']"
      ],
      "id": "informal-ottawa",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plastic-master",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af7bb32-1d3d-4a6e-8632-28af2cb09d93"
      },
      "source": [
        "# separando os casos rotulados como normais e anormais\n",
        "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
        "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
        "\n",
        "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
      ],
      "id": "plastic-master",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de dados após a filtração: 13948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secure-alignment"
      },
      "source": [
        "# removendo as imagens repetidas\n",
        "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "\n",
        "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
        "normal_data['target'] = 'normal'\n",
        "abnormal_data['target'] = 'abnormal'"
      ],
      "id": "secure-alignment",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sudden-platform",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2666eace-0379-422e-e607-5cc4b5e5c4dc"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "sudden-platform",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 10606\n",
            "quantidade de dados rotulados como anormais: 3342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prompt-embassy"
      },
      "source": [
        "# removendo 69% dos casos normais para balancear os dados\n",
        "normal, _ = train_test_split(normal_data, test_size = 0.69, random_state = 42)"
      ],
      "id": "prompt-embassy",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sitting-works",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae68f7c-f928-40cb-d982-f0b57649a280"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "sitting-works",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 3287\n",
            "quantidade de dados rotulados como anormais: 3342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funny-document"
      },
      "source": [
        "# concatenando os dataframes de casos normais e anormais\n",
        "full_data = pd.concat([normal, abnormal_data])"
      ],
      "id": "funny-document",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "golden-reading"
      },
      "source": [
        "# misturando todos os dados do dataframe e reiniciando os valores dos índices \n",
        "full_data = full_data.sample(frac = 1, axis = 0, random_state = 42).reset_index(drop=True)"
      ],
      "id": "golden-reading",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpha-poster"
      },
      "source": [
        "full_data = full_data[full_data.class_name != 'No finding']\n",
        "full_data = full_data[full_data.class_name != 'Atelectasis']\n",
        "full_data = full_data[full_data.class_name != 'Pneumothorax']"
      ],
      "id": "alpha-poster",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqVhndhsR0lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd6fa04-ccfd-4873-eba9-e39731d0d13c"
      },
      "source": [
        "full_data['class_name'].value_counts()"
      ],
      "id": "lqVhndhsR0lo",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pleural thickening    908\n",
              "Pulmonary fibrosis    746\n",
              "Lung Opacity          448\n",
              "Nodule/Mass           347\n",
              "Pleural effusion      332\n",
              "Infiltration          169\n",
              "Calcification         167\n",
              "ILD                   152\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medium-brand"
      },
      "source": [
        "# separando os dados de treinamento e de teste\n",
        "train_df, test_df = train_test_split(full_data, stratify = full_data['class_name'],\n",
        "                                     test_size = 0.2, random_state = 42)"
      ],
      "id": "medium-brand",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extraordinary-retrieval"
      },
      "source": [
        "# separando os dados de validação dos dados de treinamento\n",
        "train_df, validation_df = train_test_split(train_df, stratify = train_df['class_name'],\n",
        "                                           test_size = 0.2, random_state = 42)"
      ],
      "id": "extraordinary-retrieval",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-collins",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348c790b-a909-4cfc-cf68-2fdde732469e"
      },
      "source": [
        "# visualizando a quantidade de dados\n",
        "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
        "print('quantidade de rótulos de treinamento:', len(train_df['class_name']))\n",
        "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
        "print('quantidade de rótulos de teste:', len(test_df['class_name']))\n",
        "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
        "print('quantidade de rótulos de validação:', len(validation_df['class_name']))"
      ],
      "id": "dependent-collins",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de imagens de treinamento: 2092\n",
            "quantidade de rótulos de treinamento: 2092\n",
            "quantidade de imagens de teste: 654\n",
            "quantidade de rótulos de teste: 654\n",
            "quantidade de imagens de validação: 523\n",
            "quantidade de rótulos de validação: 523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFEnCluXpVS3"
      },
      "source": [
        "# organizando um dicionário para realizar o balanceamento nos dados das classes\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_df['class_name']),\n",
        "                                                  train_df['class_name'])\n",
        "class_weight = {0: class_weights[0], 1: class_weights[1], 2: class_weights[2], \n",
        "                3: class_weights[3], 4: class_weights[4], 5: class_weights[5],\n",
        "                6: class_weights[6], 7: class_weights[7]}"
      ],
      "id": "zFEnCluXpVS3",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "requested-pakistan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a321cc4a-2414-41a2-c49c-3ef59f071992"
      },
      "source": [
        "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
        "image_generator = ImageDataGenerator()\n",
        "\n",
        "# criando o gerador de imagens de treinamento \n",
        "train_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = train_df,\n",
        "                                                      directory = '',\n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "# criando o gerador de imagens de validação \n",
        "valid_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = validation_df,\n",
        "                                                      directory = '.', \n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "# normalizando as imagens de teste \n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "                                                  dataframe = test_df, \n",
        "                                                  directory = '.',\n",
        "                                                  x_col = 'image_path',\n",
        "                                                  y_col = 'class_name',\n",
        "                                                  batch_size = 32,\n",
        "                                                  seed = 42,\n",
        "                                                  shuffle = True,\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  target_size = (256, 256))"
      ],
      "id": "requested-pakistan",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2092 validated image filenames belonging to 8 classes.\n",
            "Found 523 validated image filenames belonging to 8 classes.\n",
            "Found 654 validated image filenames belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compatible-fetish"
      },
      "source": [
        "### Preparando a rede neural convolucional"
      ],
      "id": "compatible-fetish"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anonymous-rotation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9123a7e5-14f2-407c-f8e0-effcdbd17b61"
      },
      "source": [
        "# baixando os pesos treinados da rede inception\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "id": "anonymous-rotation",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 02:27:07--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.70.128, 74.125.132.128, 74.125.201.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.70.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  62.3MB/s    in 1.3s    \n",
            "\n",
            "2021-05-07 02:27:08 (62.3 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compact-knife"
      },
      "source": [
        "# referenciando o local em que os pesos estão armazenados\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "# carregando a arquitetura inception pré-treinada\n",
        "pre_trained_model = InceptionV3(input_shape = (256, 256, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "# carregando os pesos treinados com outros dados \n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# definindo as flags iniciais  \n",
        "#pre_trained_model.trainable = True\n",
        "#set_trainable = False\n",
        "\n",
        "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed8'\n",
        "#for layer in pre_trained_model.layers:\n",
        "#    if layer.name == 'mixed8':\n",
        "#        set_trainable = True\n",
        "#    if set_trainable:\n",
        "#        layer.trainable = True\n",
        "#    else:\n",
        "#       layer.trainable = False\n",
        "\n",
        "# visualizando a arquitetura definida\n",
        "#pre_trained_model.summary()\n",
        "\n",
        "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_output = last_layer.output"
      ],
      "id": "compact-knife",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "double-sound"
      },
      "source": [
        "# definindo uma camada de achatamento\n",
        "x = layers.Flatten()(last_output)\n",
        "# conecatando a rede uma camada com 1024 neurônios e função de ativação relu\n",
        "x = layers.Dense(units = 512, activation = 'relu')(x)     \n",
        "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
        "x = layers.Dense(units = 256, activation = 'relu')(x) \n",
        "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
        "x = layers.Dropout(rate = 0.2)(x)                  \n",
        "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
        "x = layers.Dense  (units = 8, activation = 'softmax')(x)           \n",
        "\n",
        "# conecatando as camadas definidas acima com a arquitetura inception\n",
        "model = Model(pre_trained_model.input, x) \n",
        "\n",
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "id": "double-sound",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "found-baker"
      },
      "source": [
        "# definindo o caminho pelo qual os pesos serão armazenados \n",
        "filepath = \"transferlearning_weights.hdf5\"\n",
        "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')"
      ],
      "id": "found-baker",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forward-guide"
      },
      "source": [
        "# definindo um array de callbacks\n",
        "callbacks = [checkpoint]"
      ],
      "id": "forward-guide",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vital-constitutional",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f31f4a2-1c90-499c-c147-d4cf46edc0ee"
      },
      "source": [
        "# treinando a rede neural convolucional\n",
        "history = model.fit_generator(train_generator, steps_per_epoch = 2092 // 32, \n",
        "                              validation_data = valid_generator, validation_steps = 523 // 32,\n",
        "                              callbacks = callbacks, epochs = 20, class_weight = class_weight)"
      ],
      "id": "vital-constitutional",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "65/65 [==============================] - 156s 2s/step - loss: 2.3938 - acc: 0.1328 - val_loss: 2.0318 - val_acc: 0.0781\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.07812, saving model to transferlearning_weights.hdf5\n",
            "Epoch 2/20\n",
            "65/65 [==============================] - 148s 2s/step - loss: 1.8938 - acc: 0.2239 - val_loss: 2.3133 - val_acc: 0.1406\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.07812 to 0.14062, saving model to transferlearning_weights.hdf5\n",
            "Epoch 3/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 1.2813 - acc: 0.4531 - val_loss: 2.2203 - val_acc: 0.3047\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.14062 to 0.30469, saving model to transferlearning_weights.hdf5\n",
            "Epoch 4/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 0.6473 - acc: 0.7144 - val_loss: 2.5870 - val_acc: 0.2285\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.30469\n",
            "Epoch 5/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 0.2669 - acc: 0.8627 - val_loss: 3.5858 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.30469 to 0.32812, saving model to transferlearning_weights.hdf5\n",
            "Epoch 6/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 0.1352 - acc: 0.9309 - val_loss: 4.2286 - val_acc: 0.3516\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.32812 to 0.35156, saving model to transferlearning_weights.hdf5\n",
            "Epoch 7/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 0.0826 - acc: 0.9663 - val_loss: 4.3944 - val_acc: 0.3242\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.35156\n",
            "Epoch 8/20\n",
            "65/65 [==============================] - 152s 2s/step - loss: 0.0667 - acc: 0.9636 - val_loss: 5.2800 - val_acc: 0.2617\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.35156\n",
            "Epoch 9/20\n",
            "65/65 [==============================] - 148s 2s/step - loss: 0.0654 - acc: 0.9724 - val_loss: 6.9450 - val_acc: 0.2793\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.35156\n",
            "Epoch 10/20\n",
            "65/65 [==============================] - 147s 2s/step - loss: 0.0430 - acc: 0.9835 - val_loss: 8.0319 - val_acc: 0.3066\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.35156\n",
            "Epoch 11/20\n",
            "65/65 [==============================] - 149s 2s/step - loss: 0.0208 - acc: 0.9935 - val_loss: 7.7602 - val_acc: 0.3203\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.35156\n",
            "Epoch 12/20\n",
            "65/65 [==============================] - 143s 2s/step - loss: 0.0344 - acc: 0.9839 - val_loss: 8.3337 - val_acc: 0.3027\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.35156\n",
            "Epoch 13/20\n",
            "65/65 [==============================] - 142s 2s/step - loss: 0.0442 - acc: 0.9798 - val_loss: 6.0125 - val_acc: 0.2852\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.35156\n",
            "Epoch 14/20\n",
            "65/65 [==============================] - 148s 2s/step - loss: 0.0517 - acc: 0.9818 - val_loss: 7.2572 - val_acc: 0.2852\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.35156\n",
            "Epoch 15/20\n",
            "65/65 [==============================] - 148s 2s/step - loss: 0.0496 - acc: 0.9821 - val_loss: 9.3126 - val_acc: 0.2715\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.35156\n",
            "Epoch 16/20\n",
            "65/65 [==============================] - 145s 2s/step - loss: 0.0682 - acc: 0.9774 - val_loss: 6.7199 - val_acc: 0.3223\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.35156\n",
            "Epoch 17/20\n",
            "65/65 [==============================] - 141s 2s/step - loss: 0.0403 - acc: 0.9851 - val_loss: 10.9873 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.35156\n",
            "Epoch 18/20\n",
            "65/65 [==============================] - 140s 2s/step - loss: 0.0454 - acc: 0.9835 - val_loss: 20.9099 - val_acc: 0.3086\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.35156\n",
            "Epoch 19/20\n",
            "65/65 [==============================] - 140s 2s/step - loss: 0.0492 - acc: 0.9821 - val_loss: 12.0300 - val_acc: 0.2617\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.35156\n",
            "Epoch 20/20\n",
            "65/65 [==============================] - 141s 2s/step - loss: 0.0187 - acc: 0.9894 - val_loss: 8.8200 - val_acc: 0.3027\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.35156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyCuKQpZfOB"
      },
      "source": [
        "### Salvando o modelo"
      ],
      "id": "ONyCuKQpZfOB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-CChFX2tVH2"
      },
      "source": [
        "# carregando o melhor peso obtido para o modelo\n",
        "best_model = model\n",
        "best_model.load_weights('/content/transferlearning_weights.hdf5')"
      ],
      "id": "W-CChFX2tVH2",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blslunH_y4-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7594388-9777-4927-ee70-43be29f0fa25"
      },
      "source": [
        "# salvando os dois modelos obtidos durante o treinamento\n",
        "model.save('model1')\n",
        "best_model.save('model2')"
      ],
      "id": "blslunH_y4-E",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "INFO:tensorflow:Assets written to: model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EJjwd2RZ5lW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f500da81-a561-40c3-a969-7cb8a23c042c"
      },
      "source": [
        "best_model.evaluate(test_generator)"
      ],
      "id": "2EJjwd2RZ5lW",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 8s 361ms/step - loss: 4.3026 - acc: 0.3287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.302553176879883, 0.32874616980552673]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYuik7UGZ8UA"
      },
      "source": [
        "### Métricas de avaliação do modelo"
      ],
      "id": "fYuik7UGZ8UA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YnpFy7NaAQ3"
      },
      "source": [
        "# carregando os dados de teste\n",
        "for i in range(0, 21):\n",
        "  (x1, y1) = test_generator[i]\n",
        "  if i == 0:\n",
        "    x, y = x1, y1\n",
        "  else:\n",
        "    x = np.concatenate((x, x1))\n",
        "    y = np.concatenate((y, y1))"
      ],
      "id": "5YnpFy7NaAQ3",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Mh6iQEme8U"
      },
      "source": [
        "# realizando a predição para os dados de teste\n",
        "predict = model.predict(x)\n",
        "\n",
        "count_global = 0\n",
        "for predicts in predict:\n",
        "    count = 0\n",
        "    aux = np.zeros((8,))\n",
        "    for values in predicts:\n",
        "        if values >= 0.50:\n",
        "            aux[count] = 1.\n",
        "        else:\n",
        "            aux[count] = 0.\n",
        "        count += 1\n",
        "    predict[count_global] = aux\n",
        "    count_global += 1"
      ],
      "id": "h6Mh6iQEme8U",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGqwPHeVaJDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f9000f-c5d0-466a-8516-6c26b0f60dcb"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
        "print('Matriz de Confusão:\\n', confusion_matrix(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Acurácia:', accuracy_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Precisão', precision_score(y.argmax(axis = 1), predict.argmax(axis = 1), average = 'weighted'))\n",
        "print('Sensibilidade:', recall_score(y.argmax(axis = 1), predict.argmax(axis = 1), average = 'weighted')) \n",
        "print('F1_Score:', f1_score(y.argmax(axis = 1), predict.argmax(axis = 1), average = 'weighted'))"
      ],
      "id": "hGqwPHeVaJDc",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz de Confusão:\n",
            " [[  3   0   0   0   0   0  16  14]\n",
            " [  1   1   0   2   1   0  14  11]\n",
            " [  1   0   0   1   1   2  12  17]\n",
            " [  8   0   0   4   5   8  41  24]\n",
            " [  4   0   0   1   1   2  31  31]\n",
            " [  3   0   0   4   2  18  29  10]\n",
            " [  8   0   0   2   2   7 119  44]\n",
            " [  7   0   0   3   5   5  64  65]]\n",
            "Acurácia: 0.32262996941896027\n",
            "Precisão 0.30226597728806326\n",
            "Sensibilidade: 0.32262996941896027\n",
            "F1_Score: 0.26532358658697824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4yjYhe01PFp",
        "outputId": "530eafd9-d9bc-4000-c076-a011623271fa"
      },
      "source": [
        "train_generator.class_indices"
      ],
      "id": "l4yjYhe01PFp",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Calcification': 0,\n",
              " 'ILD': 1,\n",
              " 'Infiltration': 2,\n",
              " 'Lung Opacity': 3,\n",
              " 'Nodule/Mass': 4,\n",
              " 'Pleural effusion': 5,\n",
              " 'Pleural thickening': 6,\n",
              " 'Pulmonary fibrosis': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}