{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação\n",
    "#### Entendendo as funções de ativação mais famosas em redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca numpy do python\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de ativação são uma etapa muito importante na construção de uma rede neural artificial. Com ela, é possível indicar se um determinado neurônio foi ativado ou não, existindo várias funções diferentes que se adequam melhor a determinados problemas. Abaixo, é possível ver as principais funções existentes e como é feita a sua aplicação na linguagem de programação Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![funcao-relu](Imagens/relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **ReLU** retornar valores maiores ou iguais a zero. Não existe um valor máximo de retorno, ela simplesmente vai retornar o mesmo valor se ele for igual ou maior a zero, para valores diferentes disso, a função retorna 0. Sua aplicação ocorre principalmente em **redes neurais convolucionais** e **redes neurais profundas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'ReLU'\n",
    "def reluFunction(soma):\n",
    "    if soma >= 0:\n",
    "        return soma\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplicando a função de ativação reLU\n",
    "reluFunction(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear-funcao](Imagens/linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa função de ativação não faz nenhuma ação, simplesmente retorna o mesmo número sem nenhuma restrição. Geralmente ela é usada em problemas de **regressão**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'linearFunction'\n",
    "def linearFunction(soma):\n",
    "    return soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplicando a função de ativação linear\n",
    "linearFunction(-0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax-funcao](Imagens/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativaçaõ **softmax** retorna probabilidades em problemas que contém mais de duas classes envolvidas. Geralmente é muito útil para problemas que envolvem múltiplas **classes** em **classificação**. A sua fórmula é dada por:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\frac{e(x)}{\\sum e(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'softmaxFunction'\n",
    "def softmaxFunction(x):\n",
    "    ex = np.exp(x)\n",
    "    return ex / ex.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplicando a função de ativação softmax\n",
    "softmaxFunction([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stepFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step-funcao](Imagens/step.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa função é utilizada em problemas que são **linearmente separáveis**, em que irá retornar valores discretos 0 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'stepFunction'\n",
    "def stepFunction(soma):\n",
    "    if (soma >= 1):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# aplicando a função de ativação degrau\n",
    "print(stepFunction(0))\n",
    "print(stepFunction(-1))\n",
    "print(stepFunction(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoidFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid-funcao](Imagens/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativação **sigmoidFunction** é muito utilizada em problemas de **classificação binária** e em problemas que não são **linearmente separáveis**, sua fórmula pode ser expressa por:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'sigmoidFunction'\n",
    "def sigmoidFunction(soma):\n",
    "    return 1 / (1 + np.exp(-soma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9990889488055994\n",
      "0.0009110511944006454\n"
     ]
    }
   ],
   "source": [
    "# aplicando a função de ativação sigmoide\n",
    "print(sigmoidFunction(0))\n",
    "print(sigmoidFunction(7))\n",
    "print(sigmoidFunction(-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tahnFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tahn-funcao](Imagens/tahn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativação **tahnFunction** é muito utilizada em problemas de classificação com duas classes, sua fórmula pode ser expressa por:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\frac{e^{x}-e^{-x}}{e^{-x}+e^{x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de ativação 'tahnFunction'\n",
    "def tahnFunction(soma):\n",
    "    return (np.exp(soma) - np.exp(-soma)) / (np.exp(-soma) + np.exp(soma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9993292997390669\n",
      "-0.9993292997390669\n"
     ]
    }
   ],
   "source": [
    "# aplicando a função de ativação tangente\n",
    "print(tahnFunction(0))\n",
    "print(tahnFunction(4))\n",
    "print(tahnFunction(-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No site do **Keras** é possível ver mais funções de ativação que a plataforma disponibiliza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Acesse o site do Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.4114437956141708\n",
      "-0.3434511648102781\n",
      "0.358\n",
      "-0.358\n",
      "[0.99001676 0.00667068 0.00331256]\n"
     ]
    }
   ],
   "source": [
    "# testando cada uma das funções de ativação exemplificadas\n",
    "print(stepFunction(-1))\n",
    "print(sigmoidFunction(-0.358))\n",
    "print(tahnFunction(-0.358))\n",
    "print(reluFunction(0.358))\n",
    "print(linearFunction(-0.358))\n",
    "valores = [7.0, 2.0, 1.3]\n",
    "print(softmaxFunction(valores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
